{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Polaris Leaderboard Submission Pipeline\n",
        "\n",
        "This notebook:\n",
        "1. Loads the pretrained 15M small dataset model\n",
        "2. Loads a Polaris benchmark dataset\n",
        "3. Performs 5-fold cross validation with early stopping to determine optimal training epochs\n",
        "4. Trains the final model on the complete training set\n",
        "5. Makes predictions on the test set\n",
        "6. Submits results to Polaris leaderboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polaris as po\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import os\n",
        "\n",
        "# Deep learning imports\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import custom modules\n",
        "from molencoder.utils.callbacks import BestEpochTracker\n",
        "from molencoder.finetune.dataset_preprocessor import preprocess_polaris_benchmark\n",
        "from molencoder.finetune.label_scaler import LabelScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"fabikru/model_15M_small_ds_masking_30_predicted_hparams\"\n",
        "BENCHMARK_NAME = \"tdcommons/caco2-wang\"  # Change this to your desired benchmark\n",
        "N_SPLITS = 5  # Number of cross-validation folds\n",
        "SMILES_COLUMN = \"smiles\"\n",
        "MAX_LENGTH = 500\n",
        "OWNER_NAME = \"fabikru\"  # Change this to your Polaris username\n",
        "\n",
        "# Fixed hyperparameters (based on your existing code)\n",
        "TRAINING_CONFIG = {\n",
        "    \"per_device_train_batch_size\": 64,\n",
        "    \"per_device_eval_batch_size\": 64,\n",
        "    \"learning_rate\": 8e-4,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"warmup_steps\": 100,\n",
        "    \"optim\": \"schedule_free_adamw\",\n",
        "    \"lr_scheduler_type\": \"constant\",\n",
        "    \"adam_beta1\": 0.9,\n",
        "    \"adam_beta2\": 0.999,\n",
        "    \"adam_epsilon\": 1e-8,\n",
        "    \"fp16\": False,\n",
        "    \"bf16\": True,\n",
        "    \"dataloader_num_workers\": 8,\n",
        "    \"dataloader_pin_memory\": True,\n",
        "    \"tf32\": True,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"early_stopping_patience\": 3,\n",
        "    \"early_stopping_threshold\": 0.001,\n",
        "}\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Benchmark: {BENCHMARK_NAME}\")\n",
        "print(f\"Cross-validation folds: {N_SPLITS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load and Preprocess Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Polaris benchmark\n",
        "logger.info(f\"Loading benchmark: {BENCHMARK_NAME}\")\n",
        "benchmark = po.load_benchmark(BENCHMARK_NAME)\n",
        "\n",
        "# Get train and test splits\n",
        "train_data, test_data = benchmark.get_train_test_split()\n",
        "\n",
        "# Convert to DataFrames and then to Hugging Face datasets\n",
        "train_df = train_data.as_dataframe()\n",
        "test_df = test_data.as_dataframe()\n",
        "\n",
        "print(f\"Train set size: {len(train_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "print(f\"Train columns: {train_df.columns.tolist()}\")\n",
        "print(f\"Train data sample:\")\n",
        "print(train_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "print(f\"Train dataset columns: {train_dataset.column_names}\")\n",
        "print(f\"Test dataset columns: {test_dataset.column_names}\")\n",
        "\n",
        "# Determine column names from the train dataset (test might not have labels)\n",
        "def determine_columns(dataset):\n",
        "    \"\"\"Determine SMILES and label columns from the dataset\"\"\"\n",
        "    row = dataset[0]\n",
        "    cols = list(row.keys())\n",
        "    \n",
        "    smiles_col, label_col = None, None\n",
        "    for col in cols:\n",
        "        if isinstance(row[col], str) and ('smiles' in col.lower() or 'mol' in col.lower()):\n",
        "            smiles_col = col\n",
        "        elif isinstance(row[col], (int, float)) and col != '__index_level_0__':\n",
        "            label_col = col\n",
        "    \n",
        "    if smiles_col is None:\n",
        "        # Fallback: assume first string column is SMILES\n",
        "        for col in cols:\n",
        "            if isinstance(row[col], str):\n",
        "                smiles_col = col\n",
        "                break\n",
        "    \n",
        "    if label_col is None:\n",
        "        # Fallback: assume first numeric column is labels\n",
        "        for col in cols:\n",
        "            if isinstance(row[col], (int, float)) and col != '__index_level_0__':\n",
        "                label_col = col\n",
        "                break\n",
        "    \n",
        "    return smiles_col, label_col\n",
        "\n",
        "smiles_col, label_col = determine_columns(train_dataset)\n",
        "print(f\"SMILES column: {smiles_col}\")\n",
        "print(f\"Label column: {label_col}\")\n",
        "\n",
        "# Rename SMILES column if needed (both datasets should have this)\n",
        "if smiles_col and smiles_col != \"smiles\":\n",
        "    if smiles_col in train_dataset.column_names:\n",
        "        train_dataset = train_dataset.rename_column(smiles_col, \"smiles\")\n",
        "    if smiles_col in test_dataset.column_names:\n",
        "        test_dataset = test_dataset.rename_column(smiles_col, \"smiles\")\n",
        "\n",
        "# Rename label column if needed (only train dataset should have this)\n",
        "if label_col and label_col != \"labels\":\n",
        "    if label_col in train_dataset.column_names:\n",
        "        train_dataset = train_dataset.rename_column(label_col, \"labels\")\n",
        "    # Don't try to rename in test dataset - it might not have labels\n",
        "\n",
        "# Remove any unwanted columns from train dataset\n",
        "wanted_columns_train = [\"smiles\", \"labels\"]\n",
        "for col in list(train_dataset.column_names):\n",
        "    if col not in wanted_columns_train:\n",
        "        train_dataset = train_dataset.remove_columns([col])\n",
        "\n",
        "# Remove any unwanted columns from test dataset (only keep smiles)\n",
        "wanted_columns_test = [\"smiles\"]\n",
        "for col in list(test_dataset.column_names):\n",
        "    if col not in wanted_columns_test:\n",
        "        test_dataset = test_dataset.remove_columns([col])\n",
        "\n",
        "print(f\"Final train dataset columns: {train_dataset.column_names}\")\n",
        "print(f\"Final test dataset columns: {test_dataset.column_names}\")\n",
        "print(f\"Sample train data: {train_dataset[0]}\")\n",
        "print(f\"Sample test data: {test_dataset[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup Label Scaling and Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize label scaler (only for train dataset)\n",
        "label_scaler = LabelScaler(train_dataset)\n",
        "print(f\"Label scaling - Median: {label_scaler.median:.4f}, IQR: {label_scaler.iqr:.4f}\")\n",
        "\n",
        "# Scale labels in train dataset only (test dataset doesn't have labels)\n",
        "train_dataset_scaled = label_scaler.scale_labels(train_dataset)\n",
        "\n",
        "# Load tokenizer and tokenize datasets\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"smiles\"], truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "train_dataset_tokenized = train_dataset_scaled.map(tokenize_function, batched=True)\n",
        "test_dataset_tokenized = test_dataset.map(tokenize_function, batched=True)  # No scaling for test\n",
        "\n",
        "print(\"Tokenization completed.\")\n",
        "print(f\"Sample tokenized train data: {train_dataset_tokenized[0]}\")\n",
        "print(f\"Sample tokenized test data: {test_dataset_tokenized[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cross-Validation to Determine Optimal Epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_fold(train_fold_dataset, val_fold_dataset, fold_num):\n",
        "    \"\"\"Train a single fold and return the best epoch number\"\"\"\n",
        "    logger.info(f\"Training fold {fold_num + 1}/{N_SPLITS}\")\n",
        "    \n",
        "    # Load fresh model for this fold\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, num_labels=1\n",
        "    )\n",
        "    \n",
        "    # Setup data collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "    \n",
        "    # Setup callbacks\n",
        "    best_epoch_tracker = BestEpochTracker()\n",
        "    early_stopping = EarlyStoppingCallback(\n",
        "        early_stopping_patience=TRAINING_CONFIG[\"early_stopping_patience\"],\n",
        "        early_stopping_threshold=TRAINING_CONFIG[\"early_stopping_threshold\"]\n",
        "    )\n",
        "    \n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=temp_dir,\n",
        "            logging_dir=temp_dir,\n",
        "            num_train_epochs=100,  # Large number, early stopping will control\n",
        "            per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
        "            per_device_eval_batch_size=TRAINING_CONFIG[\"per_device_eval_batch_size\"],\n",
        "            learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
        "            weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
        "            warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
        "            optim=TRAINING_CONFIG[\"optim\"],\n",
        "            lr_scheduler_type=TRAINING_CONFIG[\"lr_scheduler_type\"],\n",
        "            adam_beta1=TRAINING_CONFIG[\"adam_beta1\"],\n",
        "            adam_beta2=TRAINING_CONFIG[\"adam_beta2\"],\n",
        "            adam_epsilon=TRAINING_CONFIG[\"adam_epsilon\"],\n",
        "            fp16=TRAINING_CONFIG[\"fp16\"],\n",
        "            bf16=TRAINING_CONFIG[\"bf16\"],\n",
        "            dataloader_num_workers=TRAINING_CONFIG[\"dataloader_num_workers\"],\n",
        "            dataloader_pin_memory=TRAINING_CONFIG[\"dataloader_pin_memory\"],\n",
        "            tf32=TRAINING_CONFIG[\"tf32\"],\n",
        "            max_grad_norm=TRAINING_CONFIG[\"max_grad_norm\"],\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            eval_on_start=False,\n",
        "            save_total_limit=2,\n",
        "            torch_compile=False,  # Disable for short CV runs\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "        )\n",
        "        \n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_fold_dataset,\n",
        "            eval_dataset=val_fold_dataset,\n",
        "            data_collator=data_collator,\n",
        "            callbacks=[early_stopping, best_epoch_tracker],\n",
        "        )\n",
        "        \n",
        "        trainer.train()\n",
        "        \n",
        "        # Get the best epoch from the callback\n",
        "        best_epoch = best_epoch_tracker.best_epoch\n",
        "        best_loss = best_epoch_tracker.best_eval_loss\n",
        "        \n",
        "        logger.info(f\"Fold {fold_num + 1} - Best epoch: {best_epoch}, Best loss: {best_loss:.4f}\")\n",
        "        \n",
        "        return best_epoch, best_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform cross-validation\n",
        "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "best_epochs = []\n",
        "cv_losses = []\n",
        "\n",
        "# Convert to numpy for KFold splitting\n",
        "indices = np.arange(len(train_dataset_tokenized))\n",
        "\n",
        "for fold_num, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
        "    # Create fold datasets\n",
        "    train_fold_dataset = train_dataset_tokenized.select(train_idx.tolist())\n",
        "    val_fold_dataset = train_dataset_tokenized.select(val_idx.tolist())\n",
        "    \n",
        "    logger.info(f\"Fold {fold_num + 1}: Train size: {len(train_fold_dataset)}, Val size: {len(val_fold_dataset)}\")\n",
        "    \n",
        "    # Train this fold\n",
        "    best_epoch, best_loss = train_fold(train_fold_dataset, val_fold_dataset, fold_num)\n",
        "    \n",
        "    best_epochs.append(best_epoch)\n",
        "    cv_losses.append(best_loss)\n",
        "\n",
        "# Calculate average optimal epochs\n",
        "avg_best_epochs = np.mean(best_epochs)\n",
        "std_best_epochs = np.std(best_epochs)\n",
        "avg_cv_loss = np.mean(cv_losses)\n",
        "\n",
        "print(\"\\n=== Cross-Validation Results ===\")\n",
        "print(f\"Best epochs per fold: {best_epochs}\")\n",
        "print(f\"CV losses per fold: {[f'{loss:.4f}' for loss in cv_losses]}\")\n",
        "print(f\"Average best epochs: {avg_best_epochs:.1f} Â± {std_best_epochs:.1f}\")\n",
        "print(f\"Average CV loss: {avg_cv_loss:.4f}\")\n",
        "\n",
        "# Use the average as our target epochs for final training\n",
        "final_epochs = int(np.round(avg_best_epochs))\n",
        "print(f\"\\nUsing {final_epochs} epochs for final training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Train Final Model on Complete Training Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(\"Training final model on complete training dataset\")\n",
        "\n",
        "# Load fresh model for final training\n",
        "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=1\n",
        ")\n",
        "\n",
        "# Setup data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "with tempfile.TemporaryDirectory() as temp_dir:\n",
        "    final_training_args = TrainingArguments(\n",
        "        output_dir=temp_dir,\n",
        "        logging_dir=temp_dir,\n",
        "        num_train_epochs=final_epochs,\n",
        "        per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=TRAINING_CONFIG[\"per_device_eval_batch_size\"],\n",
        "        learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
        "        weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
        "        warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
        "        optim=TRAINING_CONFIG[\"optim\"],\n",
        "        lr_scheduler_type=TRAINING_CONFIG[\"lr_scheduler_type\"],\n",
        "        adam_beta1=TRAINING_CONFIG[\"adam_beta1\"],\n",
        "        adam_beta2=TRAINING_CONFIG[\"adam_beta2\"],\n",
        "        adam_epsilon=TRAINING_CONFIG[\"adam_epsilon\"],\n",
        "        fp16=TRAINING_CONFIG[\"fp16\"],\n",
        "        bf16=TRAINING_CONFIG[\"bf16\"],\n",
        "        dataloader_num_workers=TRAINING_CONFIG[\"dataloader_num_workers\"],\n",
        "        dataloader_pin_memory=TRAINING_CONFIG[\"dataloader_pin_memory\"],\n",
        "        tf32=TRAINING_CONFIG[\"tf32\"],\n",
        "        max_grad_norm=TRAINING_CONFIG[\"max_grad_norm\"],\n",
        "        eval_strategy=\"no\",  # No evaluation during final training\n",
        "        save_strategy=\"no\",  # No saving during final training\n",
        "        torch_compile=False,  # Disable compilation for small dataset\n",
        "        eval_on_start=False,\n",
        "        logging_steps=50,\n",
        "    )\n",
        "    \n",
        "    final_trainer = Trainer(\n",
        "        model=final_model,\n",
        "        args=final_training_args,\n",
        "        train_dataset=train_dataset_tokenized,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    # Train the final model\n",
        "    final_trainer.train()\n",
        "    \n",
        "    logger.info(\"Final training completed\")\n",
        "    \n",
        "    # Make predictions on test set\n",
        "    logger.info(\"Making predictions on test set\")\n",
        "    test_results = final_trainer.predict(test_dataset_tokenized)\n",
        "    \n",
        "    # Get predictions and rescale them\n",
        "    import numpy as np\n",
        "    scaled_predictions = np.array(test_results.predictions).flatten()\n",
        "    rescaled_predictions = label_scaler.scale_predictions(scaled_predictions.tolist())\n",
        "    \n",
        "    print(f\"Generated {len(rescaled_predictions)} predictions\")\n",
        "    print(f\"Sample predictions: {rescaled_predictions[:5]}\")\n",
        "    print(f\"Prediction stats - Mean: {np.mean(rescaled_predictions):.4f}, Std: {np.std(rescaled_predictions):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Evaluate and Submit to Polaris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate predictions using the benchmark\n",
        "logger.info(\"Evaluating predictions\")\n",
        "results = benchmark.evaluate(rescaled_predictions)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!polaris login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.name = \"MolEncoder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.upload_to_hub(owner=\"fabikru\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "polaris",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
