{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4382926",
   "metadata": {},
   "source": [
    "# MolEncoder Regression Fine-tuning Tutorial\n",
    "\n",
    "This notebook shows you how to fine-tune the pretrained *MolEncoder* model for regression tasks on your own molecular data.\n",
    "\n",
    "## What this notebook does:\n",
    "\n",
    "1. **Data Loading**: Load your molecular dataset with SMILES strings and target values\n",
    "2. **Data Preprocessing**: Tokenize SMILES and scale labels for stable training  \n",
    "3. **Training Duration Selection**: Use cross-validation to find optimal number of training epochs\n",
    "4. **Model Training**: Fine-tune MolEncoder on your specific regression task\n",
    "5. **Prediction & Evaluation**: Make predictions on new molecules and evaluate performance\n",
    "\n",
    "## Your data requirements:\n",
    "\n",
    "- **SMILES column**: Molecular structures as SMILES strings (e.g., 'CCO', 'CCOCC')\n",
    "- **Labels column**: Continuous target values for regression (e.g., solubility, toxicity scores)\n",
    "- **Format**: CSV, pandas DataFrame, or any format that can be converted to a Hugging Face Dataset\n",
    "\n",
    "## Getting started:\n",
    "\n",
    "1. Replace the example **training dataset** with your own data\n",
    "2. Replace the example **test dataset** with your own test data (towards the end of the )\n",
    "3. Run all cells - the notebook will guide you through the entire process!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49e990",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, we'll install all necessary dependencies and import the required libraries for fine-tuning MolEncoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary dependencies for the fine-tuning notebook\n",
    "%pip install torch transformers datasets accelerate schedulefree scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc77d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcfd4c",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "In this section, we'll load your training dataset and prepare it for fine-tuning. **You will need to add your own data loading here.** The data needs to contain SMILES strings and target values for your regression task. This code then:\n",
    "1) Tokenizes the dataset\n",
    "2) Scale the labels to be centered around 0 before training (neccessary to avoid big gradients in the beginning of training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c08a5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your training dataset (You need to insert your own dataset loading code here)\n",
    "\n",
    "# eg. as a pandas dataframe and then convert to a Hugging Face dataset\n",
    "raw_data = pd.DataFrame({\n",
    "    'smiles': ['CCO', 'CCOCC', 'CCC', 'CCCO', 'CCCN'],\n",
    "    'labels': [0.2, 0.1, 0.3, 0.25, 0.15]\n",
    "})\n",
    "dataset = Dataset.from_pandas(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4981e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your dataset is in the correct format\n",
    "assert 'smiles' in dataset.column_names, \"Dataset must contain 'smiles' column. Use dataset.rename_column('old_name', 'smiles') to rename.\"\n",
    "assert 'labels' in dataset.column_names, \"Dataset must contain 'labels' column. Use dataset.rename_column('old_name', 'labels') to rename.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2eb73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1665a6325a09402f93441688d863bbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we load the tokenizer and tokenize the dataset so the model can understand the SMILES strings\n",
    "model_name = \"fabikru/MolEncoder\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"smiles\"], truncation=True, max_length=502)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524cbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelScaler:\n",
    "    \"\"\"Scale labels using robust scaling (median and interquartile range (IQR)).\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset: Dataset):\n",
    "        labels = np.array(dataset['labels'])\n",
    "        self.median = np.median(labels)\n",
    "        q1 = np.percentile(labels, 25)\n",
    "        q3 = np.percentile(labels, 75)\n",
    "        self.iqr = q3 - q1\n",
    "        if self.iqr == 0:\n",
    "            raise ValueError(\"Most labels are the same. Cannot scale due to division by zero.\")\n",
    "    \n",
    "    def scale_labels(self, dataset: Dataset) -> Dataset:\n",
    "        \"\"\"Scale the labels using median and IQR.\"\"\"\n",
    "        labels = np.array(dataset['labels'])\n",
    "        scaled_labels = (labels - self.median) / self.iqr\n",
    "        new_dataset = dataset.remove_columns(['labels'])\n",
    "        new_dataset = new_dataset.add_column('labels', scaled_labels.tolist())\n",
    "        return new_dataset\n",
    "    \n",
    "    def scale_predictions(self, predictions: List[float]) -> List[float]:\n",
    "        \"\"\"Rescale predictions back to original scale.\"\"\"\n",
    "        predictions = np.array(predictions)\n",
    "        rescaled_predictions = predictions * self.iqr + self.median\n",
    "        return rescaled_predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32695719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scale the labels (important for stable training)\n",
    "label_scaler = LabelScaler(tokenized_dataset)\n",
    "tokenized_dataset_scaled = label_scaler.scale_labels(tokenized_dataset)\n",
    "\n",
    "# Update our dataset variable to use the scaled version\n",
    "tokenized_dataset = tokenized_dataset_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02757f30",
   "metadata": {},
   "source": [
    "## Training Duration Selection\n",
    "\n",
    "Here we use cross-validation to automatically determine the optimal number of training epochs for your dataset. This prevents overfitting and ensures the best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146152bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestEpochTracker(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.best_eval_loss = float(\"inf\")\n",
    "        self.best_epoch = None\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is None:\n",
    "            return control  # Nothing to do if no metrics provided\n",
    "\n",
    "        current_loss = metrics.get(\"eval_loss\")\n",
    "        if current_loss is not None and current_loss < self.best_eval_loss:\n",
    "            self.best_eval_loss = current_loss\n",
    "            self.best_epoch = metrics.get(\"epoch\")\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e04947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_epochs(dataset, model_name, tokenizer, n_splits=5, max_epochs=50):\n",
    "    \"\"\"\n",
    "    A bit lengthy code but all it does is use KFold cross validation to determine the optimal number of epochs to train (lowest validation loss).\n",
    "    \n",
    "    Args:\n",
    "        dataset: The tokenized dataset with 'input_ids', 'attention_mask', and 'labels'\n",
    "        model_name: The pretrained model name to fine-tune\n",
    "        tokenizer: The tokenizer (needed for DataCollator)\n",
    "        n_splits: Number of cross-validation folds\n",
    "        max_epochs: Maximum number of epochs to consider\n",
    "    \n",
    "    Returns:\n",
    "        int: The optimal number of epochs for training\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(f\"Finding optimal epochs using {n_splits}-fold cross-validation...\")\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_epochs = []\n",
    "    \n",
    "    # Convert to numpy for KFold splitting\n",
    "    indices = np.arange(len(dataset))\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"Training fold {fold_num + 1}/{n_splits}\")\n",
    "        \n",
    "        # Create fold datasets\n",
    "        train_fold = dataset.select(train_idx.tolist())\n",
    "        val_fold = dataset.select(val_idx.tolist())\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        \n",
    "        best_epoch_tracker = BestEpochTracker()\n",
    "        early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # These are the default hyperparameters that we used. Feel free to change them and optimize them further.\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=temp_dir,\n",
    "                logging_dir=temp_dir,\n",
    "                num_train_epochs=max_epochs,\n",
    "                per_device_train_batch_size=32,\n",
    "                per_device_eval_batch_size=32,\n",
    "                learning_rate=8e-4,\n",
    "                weight_decay=1e-5,\n",
    "                warmup_steps=100,\n",
    "                optim=\"schedule_free_adamw\",\n",
    "                lr_scheduler_type=\"constant\",\n",
    "                adam_beta1=0.9,\n",
    "                adam_beta2=0.999,\n",
    "                adam_epsilon=1e-8,\n",
    "                fp16=True, # Try turning this off if you get some weird errors. Swap this to bf16 if you have a GPU with bf16 support.\n",
    "                eval_strategy=\"epoch\",\n",
    "                save_strategy=\"no\",\n",
    "                max_grad_norm=1.0,\n",
    "                load_best_model_at_end=False,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                logging_steps=1,\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_fold,\n",
    "                eval_dataset=val_fold,\n",
    "                data_collator=data_collator,  \n",
    "                callbacks=[early_stopping, best_epoch_tracker],\n",
    "            )\n",
    "            \n",
    "            trainer.train()\n",
    "            best_epochs.append(best_epoch_tracker.best_epoch)\n",
    "        \n",
    "        # Clean up model to save memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    optimal_epochs = int(np.round(np.mean(best_epochs)))\n",
    "    print(f\"Best epochs per fold: {best_epochs}\")\n",
    "    print(f\"Optimal epochs: {optimal_epochs}\")\n",
    "    \n",
    "    return optimal_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab454dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal epochs using 5-fold cross-validation...\n",
      "Training fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. Falling back to non-compiled mode.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/50 00:00 < 00:08, 5.71 it/s, Epoch 4/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>1.262935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>1.283490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>1.311018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>1.344887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/50 00:00 < 00:07, 6.00 it/s, Epoch 4/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.445132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.560300</td>\n",
       "      <td>0.446407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>0.449150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.530900</td>\n",
       "      <td>0.455296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/50 00:00 < 00:07, 5.90 it/s, Epoch 4/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.484500</td>\n",
       "      <td>0.786283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.798454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.814665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.835194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/50 00:01 < 00:03, 10.44 it/s, Epoch 16/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>0.014186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.666700</td>\n",
       "      <td>0.012098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.653200</td>\n",
       "      <td>0.009654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.634900</td>\n",
       "      <td>0.007140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.612400</td>\n",
       "      <td>0.004841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>0.002973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>0.001638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.523800</td>\n",
       "      <td>0.000816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.484700</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.438200</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.384800</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.001859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/50 00:00 < 00:07, 6.45 it/s, Epoch 4/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.540400</td>\n",
       "      <td>0.523911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.535700</td>\n",
       "      <td>0.529148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.535683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.542847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epochs per fold: [1.0, 1.0, 1.0, 13.0, 1.0]\n",
      "Optimal epochs: 3\n"
     ]
    }
   ],
   "source": [
    "# Find optimal number of epochs using cross-validation\n",
    "optimal_epochs = find_optimal_epochs(\n",
    "    dataset=tokenized_dataset, \n",
    "    model_name=model_name,\n",
    "    tokenizer=tokenizer, \n",
    "    n_splits=5, \n",
    "    max_epochs=50 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba5c27",
   "metadata": {},
   "source": [
    "## Final Model Training\n",
    "\n",
    "Now we'll train the final model using the optimal number of epochs determined above. The model will be saved for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e08ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(dataset, model_name, tokenizer, epochs, output_dir=Path(\"./trained_model\")):\n",
    "    \"\"\"\n",
    "    Train the final model using the optimal number of epochs.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The tokenized dataset with scaled labels\n",
    "        model_name: The pretrained model name\n",
    "        tokenizer: The tokenizer\n",
    "        epochs: Number of epochs to train\n",
    "        output_dir: Where to save the trained model\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and trainer for making predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training final model for {epochs} epochs...\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "    \n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Training arguments for final model\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        logging_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=8e-4,\n",
    "        weight_decay=1e-5,\n",
    "        warmup_steps=100,\n",
    "        optim=\"schedule_free_adamw\",\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "        fp16=True, # Try turning this off if you get some weird errors. Swap this to bf16 if you have a GPU with bf16 support.\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"no\", # we train on all available data\n",
    "        save_total_limit=1, \n",
    "        max_grad_norm=1.0,\n",
    "        logging_steps=1,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.save_model()\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    return model, trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5761273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model for 3 epochs...\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.543300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.524900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "# Train the final model using optimal epochs\n",
    "\n",
    "output_dir = Path(\"./my_finetuned_model\")\n",
    "final_model, final_trainer = train_final_model(\n",
    "    dataset=tokenized_dataset,\n",
    "    model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=optimal_epochs,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a8e07",
   "metadata": {},
   "source": [
    "## Prediction & Evaluation\n",
    "\n",
    "Finally, we'll use our trained model to make predictions on your test data and evaluate the performance using standard regression metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfa3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(trainer, tokenized_test_data, label_scaler):\n",
    "    \"\"\"\n",
    "    Make predictions on new data and rescale them back to original scale.\n",
    "    \n",
    "    Args:\n",
    "        trainer: The trained Trainer object\n",
    "        tokenized_test_data: Tokenized test dataset (without labels)\n",
    "        label_scaler: The LabelScaler used during training\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions in original scale\n",
    "    \"\"\"\n",
    "    print(\"Making predictions...\")\n",
    "    \n",
    "    predictions = trainer.predict(tokenized_test_data)\n",
    "    scaled_predictions = predictions.predictions.flatten()\n",
    "    \n",
    "    # Rescale predictions back to original scale\n",
    "    original_scale_predictions = label_scaler.scale_predictions(scaled_predictions.tolist())\n",
    "    \n",
    "    print(f\"Made {len(original_scale_predictions)} predictions\")\n",
    "    return original_scale_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049474b",
   "metadata": {},
   "source": [
    "**You need to add your data loading logic here:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652c309405cf4c26a92ba53ea3e2b70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made 5 predictions\n",
      "\n",
      "Example Predictions:\n",
      "SMILES: CCOC\n",
      "Prediction: 0.211\n",
      "Actual: 0.200\n",
      "SMILES: CCN\n",
      "Prediction: 0.209\n",
      "Actual: 0.100\n",
      "SMILES: CCC\n",
      "Prediction: 0.212\n",
      "Actual: 0.300\n",
      "\n",
      "\n",
      "Performance Metrics:\n",
      "  MSE:  0.0049\n",
      "  RMSE: 0.0702\n",
      "  MAE:  0.0601\n",
      "  R²:   0.0136\n",
      "\n",
      "Results saved to my_finetuned_model/predictions.csv\n",
      "Saved 5 predictions\n"
     ]
    }
   ],
   "source": [
    "# Load your test dataset (You need to insert your own dataset loading code here) Both cases with and without labels work.\n",
    "\n",
    "# Option 1: With labels (prediction + evaluation)\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'smiles': ['CCOC', 'CCN', 'CCC', 'CCCO', 'CCCN'],\n",
    "    'labels': [0.2, 0.1, 0.3, 0.25, 0.15]  # Your ground truth labels. If you don't have labels, just provide the smiles column.\n",
    "})\n",
    "\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Check if labels are present\n",
    "has_labels = 'labels' in test_dataset.column_names\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"smiles\"], truncation=True, max_length=502)\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare dataset for prediction\n",
    "if has_labels:\n",
    "    ground_truth = test_data['labels'].tolist()\n",
    "    prediction_dataset = tokenized_test_dataset.remove_columns(['labels'])\n",
    "else:\n",
    "    ground_truth = None\n",
    "    prediction_dataset = tokenized_test_dataset\n",
    "\n",
    "predictions = make_predictions(final_trainer, prediction_dataset, label_scaler)\n",
    "\n",
    "# Show some example predictions\n",
    "print(f\"\\nExample Predictions:\")\n",
    "for i in range(min(3, len(test_data['smiles']))):\n",
    "    smiles = test_data['smiles'].iloc[i]\n",
    "    prediction = predictions[i]\n",
    "    \n",
    "    print(f\"SMILES: {smiles}\")\n",
    "    print(f\"Prediction: {prediction:.3f}\")\n",
    "    \n",
    "    if has_labels:\n",
    "        actual = ground_truth[i]\n",
    "        print(f\"Actual: {actual:.3f}\")\n",
    "\n",
    "# Calculate and display metrics if labels are available\n",
    "if has_labels:\n",
    "    mse = mean_squared_error(ground_truth, predictions)\n",
    "    mae = mean_absolute_error(ground_truth, predictions) \n",
    "    r2 = r2_score(ground_truth, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f\"\\n\\nPerformance Metrics:\")\n",
    "    print(f\"  MSE:  {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "\n",
    "# Create a results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'smiles': test_data['smiles'].values,\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "# Add labels column if available\n",
    "if has_labels:\n",
    "    results_df['label'] = ground_truth\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "# Save to CSV\n",
    "results_df.to_csv(str(output_dir / \"predictions.csv\"), index=False)\n",
    "print(f\"\\nResults saved to {output_dir / 'predictions.csv'}\")\n",
    "print(f\"Saved {len(results_df)} predictions\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
