{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MolEncoder Classification Fine-tuning Tutorial\n",
        "\n",
        "This notebook shows you how to fine-tune the pretrained *MolEncoder* model for classification tasks on your own molecular data.\n",
        "\n",
        "## What this notebook does:\n",
        "\n",
        "1. **Data Loading**: Load your molecular dataset with SMILES strings and class labels\n",
        "2. **Data Preprocessing**: Tokenize SMILES for model input  \n",
        "3. **Training Duration Selection**: Use cross-validation to find optimal number of training epochs\n",
        "4. **Model Training**: Fine-tune MolEncoder on your specific classification task\n",
        "5. **Prediction & Evaluation**: Make predictions on new molecules and evaluate performance\n",
        "\n",
        "## Your data requirements:\n",
        "\n",
        "- **SMILES column**: Molecular structures as SMILES strings (e.g., 'CCO', 'CCOCC')\n",
        "- **Labels column**: Integer class labels for classification (e.g., 0, 1, 2, 3... for any number of classes)\n",
        "- **Format**: CSV, pandas DataFrame, or any format that can be converted to a Hugging Face Dataset\n",
        "\n",
        "**Note**: This notebook supports binary classification (2 classes), multi-class classification (3+ classes), and any number of classes!\n",
        "\n",
        "## Getting started:\n",
        "\n",
        "1. Replace the example **training dataset** with your own data\n",
        "2. Replace the example **test dataset** with your own test data (towards the end of the notebook)\n",
        "3. Run all cells - the notebook will guide you through the entire process!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "First, we'll install all necessary dependencies and import the required libraries for fine-tuning MolEncoder:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all necessary dependencies for the fine-tuning notebook\n",
        "%pip install torch transformers datasets accelerate schedulefree scikit-learn pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    Trainer,\n",
        "    TrainerCallback,\n",
        "    TrainingArguments,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading & Preprocessing\n",
        "\n",
        "In this section, we'll load your training dataset and prepare it for fine-tuning. **You will need to add your own data loading here.** The data needs to contain SMILES strings and integer class labels for your classification task. \n",
        "This code then tokenizes the dataset for model input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load your training dataset (You need to insert your own dataset loading code here)\n",
        "\n",
        "# eg. as a pandas dataframe and then convert to a Hugging Face dataset\n",
        "# Multi-class classification example (0 = low activity, 1 = medium activity, 2 = high activity)\n",
        "raw_data = pd.DataFrame({\n",
        "    'smiles': ['CCO', 'CCOCC', 'CCC', 'CCCO', 'CCCN', 'CCCC', 'CCCCC', 'CCCCCC', 'CC', 'CCCCCCCC'],\n",
        "    'labels': [0, 1, 0, 2, 1, 0, 2, 1, 0, 2]  # Multi-class labels: 0, 1, 2\n",
        "})\n",
        "dataset = Dataset.from_pandas(raw_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 3\n",
            "Classes: [0, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "# Make sure your dataset is in the correct format\n",
        "assert 'smiles' in dataset.column_names, \"Dataset must contain 'smiles' column. Use dataset.rename_column('old_name', 'smiles') to rename.\"\n",
        "assert 'labels' in dataset.column_names, \"Dataset must contain 'labels' column. Use dataset.rename_column('old_name', 'labels') to rename.\"\n",
        "\n",
        "# Check the number of unique classes\n",
        "num_classes = len(set(dataset['labels']))\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Classes: {sorted(set(dataset['labels']))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed887c8a8c9b44b7ae628f23626036b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Now we load the tokenizer and tokenize the dataset so the model can understand the SMILES strings\n",
        "model_name = \"fabikru/MolEncoder\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"smiles\"], truncation=True, max_length=502)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Duration Selection\n",
        "\n",
        "Here we use cross-validation to automatically determine the optimal number of training epochs for your dataset. This prevents overfitting and ensures the best performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BestEpochTracker(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.best_eval_loss = float(\"inf\")\n",
        "        self.best_epoch = None\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics is None:\n",
        "            return control  # Nothing to do if no metrics provided\n",
        "\n",
        "        current_loss = metrics.get(\"eval_loss\")\n",
        "        if current_loss is not None and current_loss < self.best_eval_loss:\n",
        "            self.best_eval_loss = current_loss\n",
        "            self.best_epoch = metrics.get(\"epoch\")\n",
        "        return control\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_optimal_epochs(dataset, model_name, tokenizer, num_classes, n_splits=5, max_epochs=50):\n",
        "    \"\"\"\n",
        "    A bit lengthy code but all it does is use KFold cross validation to determine the optimal number of epochs to train (lowest validation loss).\n",
        "    \n",
        "    Args:\n",
        "        dataset: The tokenized dataset with 'input_ids', 'attention_mask', and 'labels'\n",
        "        model_name: The pretrained model name to fine-tune\n",
        "        tokenizer: The tokenizer (needed for DataCollator)\n",
        "        num_classes: Number of classes for classification\n",
        "        n_splits: Number of cross-validation folds\n",
        "        max_epochs: Maximum number of epochs to consider\n",
        "    \n",
        "    Returns:\n",
        "        int: The optimal number of epochs for training\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    print(f\"Finding optimal epochs using {n_splits}-fold cross-validation...\")\n",
        "    \n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "    \n",
        "    # Setup cross-validation\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    best_epochs = []\n",
        "    \n",
        "    # Convert to numpy for KFold splitting\n",
        "    indices = np.arange(len(dataset))\n",
        "    \n",
        "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
        "        print(f\"Training fold {fold_num + 1}/{n_splits}\")\n",
        "        \n",
        "        # Create fold datasets\n",
        "        train_fold = dataset.select(train_idx.tolist())\n",
        "        val_fold = dataset.select(val_idx.tolist())\n",
        "        \n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
        "        \n",
        "        best_epoch_tracker = BestEpochTracker()\n",
        "        early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "        \n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            # These are the default hyperparameters that we used. Feel free to change them and optimize them further.\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=temp_dir,\n",
        "                logging_dir=temp_dir,\n",
        "                num_train_epochs=max_epochs,\n",
        "                per_device_train_batch_size=32,\n",
        "                per_device_eval_batch_size=32,\n",
        "                learning_rate=8e-4,\n",
        "                weight_decay=1e-5,\n",
        "                warmup_steps=100,\n",
        "                optim=\"schedule_free_adamw\",\n",
        "                lr_scheduler_type=\"constant\",\n",
        "                adam_beta1=0.9,\n",
        "                adam_beta2=0.999,\n",
        "                adam_epsilon=1e-8,\n",
        "                fp16=True, # Try turning this off if you get some weird errors. Swap this to bf16 if you have a GPU with bf16 support.\n",
        "                eval_strategy=\"epoch\",\n",
        "                save_strategy=\"no\",\n",
        "                max_grad_norm=1.0,\n",
        "                load_best_model_at_end=False,\n",
        "                metric_for_best_model=\"eval_loss\",\n",
        "                greater_is_better=False,\n",
        "                logging_steps=1,\n",
        "            )\n",
        "            \n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_fold,\n",
        "                eval_dataset=val_fold,\n",
        "                data_collator=data_collator,  \n",
        "                callbacks=[early_stopping, best_epoch_tracker],\n",
        "            )\n",
        "            \n",
        "            trainer.train()\n",
        "            best_epochs.append(best_epoch_tracker.best_epoch)\n",
        "        \n",
        "        # Clean up model to save memory\n",
        "        del model\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    \n",
        "    optimal_epochs = int(np.round(np.mean(best_epochs)))\n",
        "    print(f\"Best epochs per fold: {best_epochs}\")\n",
        "    print(f\"Optimal epochs: {optimal_epochs}\")\n",
        "    \n",
        "    return optimal_epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finding optimal epochs using 5-fold cross-validation...\n",
            "Training fold 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
            "Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. Falling back to non-compiled mode.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4/50 00:00 < 00:10, 4.25 it/s, Epoch 4/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.115700</td>\n",
              "      <td>1.273440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.113300</td>\n",
              "      <td>1.274420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.109200</td>\n",
              "      <td>1.276269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.103400</td>\n",
              "      <td>1.279199</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training fold 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4/50 00:00 < 00:10, 4.28 it/s, Epoch 4/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.112300</td>\n",
              "      <td>1.127296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.108800</td>\n",
              "      <td>1.130762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.103100</td>\n",
              "      <td>1.135065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.095300</td>\n",
              "      <td>1.139908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training fold 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8/50 00:00 < 00:06, 6.99 it/s, Epoch 8/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.076200</td>\n",
              "      <td>1.269171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.073100</td>\n",
              "      <td>1.268849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.068100</td>\n",
              "      <td>1.268488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.061100</td>\n",
              "      <td>1.268160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.052100</td>\n",
              "      <td>1.267976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.041000</td>\n",
              "      <td>1.268084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.027700</td>\n",
              "      <td>1.268610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.011700</td>\n",
              "      <td>1.269581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training fold 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4/50 00:00 < 00:12, 3.67 it/s, Epoch 4/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.172500</td>\n",
              "      <td>0.950231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.169200</td>\n",
              "      <td>0.954692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.163900</td>\n",
              "      <td>0.960663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.156600</td>\n",
              "      <td>0.967963</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training fold 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4/50 00:00 < 00:07, 6.48 it/s, Epoch 4/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.172000</td>\n",
              "      <td>0.890116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.167800</td>\n",
              "      <td>0.896568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.160900</td>\n",
              "      <td>0.905498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.151300</td>\n",
              "      <td>0.917317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best epochs per fold: [1.0, 1.0, 5.0, 1.0, 1.0]\n",
            "Optimal epochs: 2\n"
          ]
        }
      ],
      "source": [
        "# Find optimal number of epochs using cross-validation\n",
        "optimal_epochs = find_optimal_epochs(\n",
        "    dataset=tokenized_dataset, \n",
        "    model_name=model_name,\n",
        "    tokenizer=tokenizer,\n",
        "    num_classes=num_classes,\n",
        "    n_splits=5, \n",
        "    max_epochs=50 \n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Training\n",
        "\n",
        "Now we'll train the final model using the optimal number of epochs determined above. The model will be saved for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_final_model(dataset, model_name, tokenizer, num_classes, epochs, output_dir=Path(\"./trained_model\")):\n",
        "    \"\"\"\n",
        "    Train the final model using the optimal number of epochs.\n",
        "    \n",
        "    Args:\n",
        "        dataset: The tokenized dataset\n",
        "        model_name: The pretrained model name\n",
        "        tokenizer: The tokenizer\n",
        "        num_classes: Number of classes for classification\n",
        "        epochs: Number of epochs to train\n",
        "        output_dir: Where to save the trained model\n",
        "    \n",
        "    Returns:\n",
        "        Trained model and trainer for making predictions\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"Training final model for {epochs} epochs...\")\n",
        "    \n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
        "    \n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "    \n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Training arguments for final model\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        logging_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        learning_rate=8e-4,\n",
        "        weight_decay=1e-5,\n",
        "        warmup_steps=100,\n",
        "        optim=\"schedule_free_adamw\",\n",
        "        lr_scheduler_type=\"constant\",\n",
        "        adam_beta1=0.9,\n",
        "        adam_beta2=0.999,\n",
        "        adam_epsilon=1e-8,\n",
        "        fp16=True, # Try turning this off if you get some weird errors. Swap this to bf16 if you have a GPU with bf16 support.\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"no\", # we train on all available data\n",
        "        save_total_limit=1, \n",
        "        max_grad_norm=1.0,\n",
        "        logging_steps=1,\n",
        "    )\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    trainer.save_model()\n",
        "    print(f\"Model saved to {output_dir}\")\n",
        "    \n",
        "    return model, trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training final model for 2 epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at fabikru/MolEncoder and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.114800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.112100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to my_finetuned_classification_model\n"
          ]
        }
      ],
      "source": [
        "# Train the final model using optimal epochs\n",
        "\n",
        "output_dir = Path(\"./my_finetuned_classification_model\")\n",
        "final_model, final_trainer = train_final_model(\n",
        "    dataset=tokenized_dataset,\n",
        "    model_name=model_name,\n",
        "    tokenizer=tokenizer,\n",
        "    num_classes=num_classes,\n",
        "    epochs=optimal_epochs,\n",
        "    output_dir=output_dir\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction & Evaluation\n",
        "\n",
        "Finally, we'll use our trained model to make predictions on your test data and evaluate the performance using standard classification metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_predictions(trainer, tokenized_test_data):\n",
        "    \"\"\"\n",
        "    Make predictions on new data.\n",
        "    \n",
        "    Args:\n",
        "        trainer: The trained Trainer object\n",
        "        tokenized_test_data: Tokenized test dataset (without labels)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (predicted_classes, prediction_probabilities)\n",
        "    \"\"\"\n",
        "    print(\"Making predictions...\")\n",
        "    \n",
        "    predictions = trainer.predict(tokenized_test_data)\n",
        "    \n",
        "    # For classification, get the predicted class (argmax of logits)\n",
        "    predicted_classes = np.argmax(predictions.predictions, axis=1)\n",
        "    \n",
        "    # Convert logits to probabilities using softmax\n",
        "    prediction_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "    \n",
        "    print(f\"Made {len(predicted_classes)} predictions\")\n",
        "    return predicted_classes, prediction_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You need to add your data loading logic here:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6aa6b773e864bcab4951e4caf569399",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Made 5 predictions\n",
            "\n",
            "Example Predictions:\n",
            "SMILES: CCOC\n",
            "Predicted Class: 2\n",
            "  Class 0 probability: 0.323\n",
            "  Class 1 probability: 0.271\n",
            "  Class 2 probability: 0.406\n",
            "Actual Class: 1\n",
            "\n",
            "SMILES: CCN\n",
            "Predicted Class: 2\n",
            "  Class 0 probability: 0.319\n",
            "  Class 1 probability: 0.313\n",
            "  Class 2 probability: 0.368\n",
            "Actual Class: 0\n",
            "\n",
            "SMILES: CCCCO\n",
            "Predicted Class: 2\n",
            "  Class 0 probability: 0.353\n",
            "  Class 1 probability: 0.240\n",
            "  Class 2 probability: 0.406\n",
            "Actual Class: 2\n",
            "\n",
            "\n",
            "Performance Metrics:\n",
            "  Accuracy:  0.2000\n",
            "  Precision: 0.0400\n",
            "  Recall:    0.2000\n",
            "  F1-score:  0.0667\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       0.20      1.00      0.33         1\n",
            "\n",
            "    accuracy                           0.20         5\n",
            "   macro avg       0.07      0.33      0.11         5\n",
            "weighted avg       0.04      0.20      0.07         5\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[0 0 2]\n",
            " [0 0 2]\n",
            " [0 0 1]]\n",
            "\n",
            "Results saved to my_finetuned_classification_model/predictions.csv\n",
            "Saved 5 predictions\n"
          ]
        }
      ],
      "source": [
        "# Load your test dataset (You need to insert your own dataset loading code here) Both cases with and without labels work.\n",
        "\n",
        "# Option 1: With labels (prediction + evaluation)\n",
        "\n",
        "test_data = pd.DataFrame({\n",
        "    'smiles': ['CCOC', 'CCN', 'CCCCO', 'CCCCN', 'CCCCCCC'],\n",
        "    'labels': [1, 0, 2, 1, 0]  # Your ground truth class labels (matching the training classes). If you don't have labels, just provide the smiles column.\n",
        "})\n",
        "\n",
        "\n",
        "test_dataset = Dataset.from_pandas(test_data)\n",
        "\n",
        "# Check if labels are present\n",
        "has_labels = 'labels' in test_dataset.column_names\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"smiles\"], truncation=True, max_length=502)\n",
        "\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepare dataset for prediction\n",
        "if has_labels:\n",
        "    ground_truth = test_data['labels'].tolist()\n",
        "    prediction_dataset = tokenized_test_dataset.remove_columns(['labels'])\n",
        "else:\n",
        "    ground_truth = None\n",
        "    prediction_dataset = tokenized_test_dataset\n",
        "\n",
        "predicted_classes, prediction_probs = make_predictions(final_trainer, prediction_dataset)\n",
        "\n",
        "# Show some example predictions\n",
        "print(f\"\\nExample Predictions:\")\n",
        "for i in range(min(3, len(test_data['smiles']))):\n",
        "    smiles = test_data['smiles'].iloc[i]\n",
        "    prediction = predicted_classes[i]\n",
        "    \n",
        "    print(f\"SMILES: {smiles}\")\n",
        "    print(f\"Predicted Class: {prediction}\")\n",
        "    \n",
        "    # Show probabilities for each class\n",
        "    for class_idx in range(num_classes):\n",
        "        prob = prediction_probs[i][class_idx]\n",
        "        print(f\"  Class {class_idx} probability: {prob:.3f}\")\n",
        "    \n",
        "    if has_labels:\n",
        "        actual = ground_truth[i]\n",
        "        print(f\"Actual Class: {actual}\")\n",
        "    print()\n",
        "\n",
        "# Calculate and display metrics if labels are available\n",
        "if has_labels:\n",
        "    accuracy = accuracy_score(ground_truth, predicted_classes)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(ground_truth, predicted_classes, average='weighted', zero_division=0)\n",
        "    \n",
        "    print(f\"\\nPerformance Metrics:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-score:  {f1:.4f}\")\n",
        "    \n",
        "    # Detailed classification report\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(ground_truth, predicted_classes, zero_division=0))\n",
        "    \n",
        "    # Confusion matrix\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(ground_truth, predicted_classes))\n",
        "\n",
        "# Create a results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'smiles': test_data['smiles'].values,\n",
        "    'predicted_class': predicted_classes\n",
        "})\n",
        "\n",
        "# Add probability columns for each class\n",
        "for class_idx in range(num_classes):\n",
        "    results_df[f'class_{class_idx}_probability'] = prediction_probs[:, class_idx]\n",
        "\n",
        "# Add labels column if available\n",
        "if has_labels:\n",
        "    results_df['true_class'] = ground_truth\n",
        "\n",
        "# Save to CSV\n",
        "results_df.to_csv(str(output_dir / \"predictions.csv\"), index=False)\n",
        "print(f\"\\nResults saved to {output_dir / 'predictions.csv'}\")\n",
        "print(f\"Saved {len(results_df)} predictions\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "encoder",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
